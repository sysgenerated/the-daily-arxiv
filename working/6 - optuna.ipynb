{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would I subjectively say one set of clusters are better than another?\n",
    "* the cluster label/phrase accurately describes the cluster\n",
    "* the cluster labels/phrases are distinct\n",
    "* the noise is minimized\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-03 14:25:17,913] A new study created in memory with name: no-name-0a519b4f-5fe5-4f55-b317-af47b53f4049\n",
      "[I 2024-03-03 14:25:21,624] Trial 0 finished with value: 413.0 and parameters: {'n_neighbors': 30, 'min_dist': 0.9673649701713906, 'n_components': 11, 'preprocess_pca': 'False', 'min_cluster_size': 15, 'min_samples': 17, 'cluster_selection_epsilon': 0.38672734734204406, 'cluster_selection_method': 'eom'}. Best is trial 0 with value: 413.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "85.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 30,\n",
       " 'min_dist': 0.9673649701713906,\n",
       " 'n_components': 11,\n",
       " 'preprocess_pca': 'False',\n",
       " 'min_cluster_size': 15,\n",
       " 'min_samples': 17,\n",
       " 'cluster_selection_epsilon': 0.38672734734204406,\n",
       " 'cluster_selection_method': 'eom'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import umap\n",
    "import hdbscan\n",
    "import optuna\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "def pca_preprocessor(data):\n",
    "    n_components_max = len(data)\n",
    "    pca = PCA(n_components=n_components_max)\n",
    "    pca.fit(data)\n",
    "   \n",
    "    explained_variance_ratio_ = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance_ratio_ = explained_variance_ratio_.cumsum()\n",
    "   \n",
    "    # Calculate the second derivative\n",
    "    second_derivative = np.gradient(np.gradient(cumulative_explained_variance_ratio_))\n",
    "    # Find the index of the maximum of the second derivative\n",
    "    recommended_components_elbow = np.argmax(second_derivative)\n",
    "   \n",
    "    # Alternative recommendation based on 90% explained variance\n",
    "    threshold = 0.9\n",
    "    recommended_components_90_pct = None  # Initialize outside the loop\n",
    "    for i, ratio in enumerate(cumulative_explained_variance_ratio_):\n",
    "        if ratio >= threshold:\n",
    "            recommended_components_90_pct = i + 1\n",
    "            break\n",
    "      \n",
    "    selected_components = min(recommended_components_elbow, recommended_components_90_pct)\n",
    "    pca = PCA(n_components=selected_components)\n",
    "\n",
    "    return pca.fit_transform(data)\n",
    "\n",
    "\n",
    "def umap_reduce(data, n_neighbors=30, min_dist=0.0, n_components=10, metric='cosine', preprocess_pca=True):\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric\n",
    "        )\n",
    "    \n",
    "    if preprocess_pca:\n",
    "       data = pca_preprocessor(data)\n",
    "    return reducer.fit_transform(data)\n",
    "\n",
    "\n",
    "def get_abstracts(cluster_id, probability, abstract, use_top_three=True):\n",
    "    # Convert lists to numpy arrays\n",
    "    cluster_id = np.array(cluster_id)\n",
    "    probability = np.array(probability)\n",
    "    abstract = np.array(abstract)\n",
    "    \n",
    "    # Get unique cluster ids\n",
    "    unique_clusters = np.unique(cluster_id)\n",
    "    \n",
    "    # Create a dictionary to hold the selected abstracts\n",
    "    selected_abstracts = {}\n",
    "    \n",
    "    for cluster in unique_clusters:\n",
    "        # Get indices of cluster entries\n",
    "        cluster_indices = np.where(cluster_id == cluster)[0]\n",
    "        \n",
    "        # Sort probabilities for this cluster\n",
    "        sorted_indices = np.argsort(probability[cluster_indices])[::-1]\n",
    "        \n",
    "        if use_top_three:\n",
    "            selected_indices = sorted_indices[:3]\n",
    "        else:\n",
    "            remaining_indices = sorted_indices[3:]\n",
    "            selected_indices = random.sample(remaining_indices.tolist(), min(3, len(remaining_indices)))\n",
    "        \n",
    "        selected_abstracts[cluster] = abstract[cluster_indices[selected_indices]].tolist()\n",
    "    \n",
    "    return selected_abstracts\n",
    "\n",
    "\n",
    "def format_initial_prompt(selected_abstracts):\n",
    "    return f\"Use the following data which is in the format <cluster_id>: [articles] to generate \\\n",
    "a single word and single sentence summary of the type of information the cluster represents. \\\n",
    "The single world and single sentence should uniquely identify a given cluster. \\\n",
    "--- Abstracts --- \\\n",
    " {selected_abstracts} \\\n",
    " -------- \\\n",
    "The returned values should only be in the format <cluster_id>: (word, sentence). \"\n",
    "\n",
    "\n",
    "def format_evaluation_prompt(selected_abstracts, generated_summaries):\n",
    "    return f\"You are provided with grouped article abstracts in the format <cluster_id>: [list of abstracts]. \\\n",
    "You are also provided with a single word and single sentence proposed summary of the clusters in the format \\\n",
    "<cluster_id>: (single word, single sentence). On a scale of 1 to 100 with 1 being worst and 100 being best \\\n",
    "evaluate the semantic accuracy of the summary and uniqueness of the summary per cluster. \\\n",
    "--- Abstracts --- \\\n",
    "{selected_abstracts} \\\n",
    "--- Summaries --- \\\n",
    "{generated_summaries} \\\n",
    "------ \\\n",
    "Your answer should only be in the form: <cluster_id>: rating. \"\n",
    "\n",
    "\n",
    "def retrieve_llm_score(cluster_ids, probabilities, abstracts):\n",
    "    # Retreive 3 articles from each cluster closest to the center\n",
    "    articles = get_abstracts(cluster_ids, probabilities, abstracts, use_top_three=True)\n",
    "    # Stuff articles in prompt\n",
    "    initial_prompt = format_initial_prompt(articles)\n",
    "    # Retreive summaries from the LLM\n",
    "    example_generated_summaries = \"-1: ('Quantum', 'This cluster focuses on the generation of realistic-looking quantum circuits to enhance benchmarking and advance the development of quantum compilers and hardware.')\\\n",
    "0: ('Reasoning', 'This cluster revolves around evaluating large language models' abilities in critique and rectify reasoning across various domains using the CriticBench benchmark.')\\\n",
    "1: ('Graph', 'The topics in this cluster include the expansion of polytopes, finding Hamiltonian cycles in graphs, and embedding Poincar\\'e halfspace into discrete metric spaces.')\\\n",
    "2: ('Vision', 'This cluster discusses novel approaches and applications in computer vision, including oriented object detection with Vision Transformers and controllable video editing for seamless object insertion in videos.')\"\n",
    "    # Retreive 3 other articles randomly from cluster\n",
    "    articles = get_abstracts(cluster_ids, probabilities, abstracts, use_top_three=False)\n",
    "    # Stuff the articles in a prompt\n",
    "    evaluation_prompt = format_evaluation_prompt(articles, example_generated_summaries)\n",
    "    # Retrive a score from the LLM\n",
    "    example_llm_output = {-1: 90,\n",
    "                          0: 85,\n",
    "                          1: 80,\n",
    "                          2: 85}\n",
    "    return np.mean(list(example_llm_output.values()))\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    n_neighbors = trial.suggest_int(\"n_neighbors\", 20, 40)\n",
    "    min_dist = trial.suggest_float(\"min_dist\", 0.0, 1.0)\n",
    "    n_components = trial.suggest_int(\"n_components\", 10, 20)\n",
    "    preprocess_pca = trial.suggest_categorical(\"preprocess_pca\", [\"True\", \"False\"])\n",
    "\n",
    "    min_cluster_size = trial.suggest_int(\"min_cluster_size\", 10, 30)\n",
    "    min_samples = trial.suggest_int(\"min_samples\", 5, 30)\n",
    "    cluster_selection_epsilon = trial.suggest_float(\"cluster_selection_epsilon\", 0.0, 1.0)\n",
    "    cluster_selection_method = trial.suggest_categorical(\"cluster_selection_method\", [\"eom\", \"leaf\"])\n",
    "\n",
    "    data = df[\"embeddings\"].to_list()\n",
    "    data = umap_reduce(data, \n",
    "                    n_neighbors=n_neighbors, \n",
    "                    min_dist=min_dist, \n",
    "                    n_components=n_components, \n",
    "                    preprocess_pca=preprocess_pca)\n",
    "\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                            min_samples=min_samples,\n",
    "                            cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "                            metric=\"euclidean\",\n",
    "                            cluster_selection_method=cluster_selection_method,\n",
    "                            prediction_data=True)\n",
    "    \n",
    "    clusterer.fit(data)\n",
    "\n",
    "    cluster_ids = clusterer.labels_\n",
    "    print(cluster_ids.max()+2)\n",
    "    probabilities = clusterer.probabilities_\n",
    "    abstracts = df[\"Abstract\"].to_list()\n",
    "\n",
    "    llm_score = retrieve_llm_score(cluster_ids, probabilities,abstracts)\n",
    "    print(llm_score)\n",
    "\n",
    "    noise_count = sum(clusterer.labels_ == -1)\n",
    "\n",
    "    threshold = 0.05\n",
    "    low_prob_count = probabilities[probabilities <= threshold].sum()\n",
    "\n",
    "    value = noise_count + low_prob_count\n",
    "    return value\n",
    "\n",
    "\n",
    "df = pd.read_pickle(\"./archive/arxiv_step2.pkl\")\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "study.best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novel2comic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
