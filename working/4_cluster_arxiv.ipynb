{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define UMAP / HDBSCAN functions\n",
    "\n",
    "https://github.com/dborrelli/chat-intents\n",
    "\n",
    "1.  n_neighbors\n",
    "\n",
    "* A suitable starting range for n_neighbors with sentence embeddings is between 5 and 50. Here's why:\n",
    "\n",
    "  * Lo*wer bound (5): Prevents overly localized clusters and ensures a minimum level of connectivity between data points.\n",
    "  * Upper bound (50): Avoids creating clusters that are too broad and lack meaningful semantic distinctions.\n",
    "\n",
    "* Factors Affecting the Range\n",
    "\n",
    "  * Dataset size: Larger datasets might tolerate slightly larger n_neighbors ranges before clusters become too broad.\n",
    "  * Embedding quality: Well-trained embeddings with clear semantic distinctions might benefit from slightly smaller n_neighbors 2. \n",
    "2.  min_dist\n",
    "\n",
    "* A suitable starting range for min_dist is between 0.0 and 0.5.  Here's the reasoning\n",
    "  * Lower bound (0.0): Allows for the possibility of dense, overlapping clusters if the data structure warrants it.\n",
    "  * Upper bound (0.5): Prevents clusters from becoming excessively compressed, aiding visual interpretation and potentially improving downstream clustering results.\n",
    "\n",
    "* Generally, you would use a lower min_dist with a smaller n_neighbors and vice versa.\n",
    "\n",
    "3.  n_components\n",
    "\n",
    "* A suitable starting range for n_components is between 5 and 15. Here's the reasoning:\n",
    "* Minimum for HDBSCAN: HDBSCAN benefits from having some room to identify varying densities within the data. A minimum of around 5 dimensions generally provides enough space to form more nuanced clusters.\n",
    "\n",
    "4.  metric\n",
    "\n",
    "* A suitable metric is cosine.\n",
    "\n",
    "  * Cosine: Cosine distance is particularly suitable for sentence embedding clustering due to its focus on directionality, inherent normalization, and sparsity preservation.\n",
    "\n",
    "  * Mahalanobis: This metric takes into account the covariance structure of the data, making it effective when the data exhibits different variances or correlations across features. While it might not be the most intuitive choice for sentence embeddings, it can be a good option if the embeddings have complex relationships between features.\n",
    "\n",
    "  * Euclidean: This is the default metric used by UMAP and is a common choice for many clustering tasks. However, for sentence embeddings, it might not be as effective as cosine distance because it prioritizes absolute differences in magnitudes which might not capture semantic similarity well.\n",
    "\n",
    "  * Minkowski (including Manhattan and Chebyshev): These metrics are generalizations of Euclidean distance, where Manhattan distance uses the sum of absolute differences and Chebyshev distance uses the maximum absolute difference. While they might be suitable for specific scenarios, they generally lack the interpretability and effectiveness of cosine distance for sentence embeddings.\n",
    "\n",
    "  * Miscellaneous spatial metrics (Canberra, Braycurtis, Haversine): These metrics are designed for specific purposes like measuring similarity between categorical data (Canberra, Braycurtis) or geographical locations (Haversine). They are not well-suited for the task of sentence embedding clustering.\n",
    "\n",
    "  * Normalized spatial metrics (wminkowski, seuclidean): These are variants of Minkowski distance with some normalization applied. While they might address some limitations of the original metrics, they generally don't offer significant advantages over cosine distance for sentence embeddings.\n",
    "\n",
    "  * Angular and correlation metrics (Correlation): While correlation measures the linear relationship between vectors, it might not be suitable for capturing semantic similarity in sentence embeddings, which can have complex non-linear relationships.\n",
    "\n",
    "  * Metrics for binary data: These metrics (Hamming, Jaccard, Dice, etc.) are designed to compare binary data and are not applicable to sentence embeddings, which are typically represented as continuous vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def pca_preprocessor(data, plot=False):\n",
    "  n_components_max = len(data)\n",
    "  pca = PCA(n_components=n_components_max)\n",
    "  pca.fit(data)\n",
    "\n",
    "  explained_variance_ratio_ = pca.explained_variance_ratio_\n",
    "  cumulative_explained_variance_ratio_ = explained_variance_ratio_.cumsum()\n",
    "\n",
    "  if plot:\n",
    "    # Plot explained variance\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, n_components_max + 1), explained_variance_ratio_)\n",
    "    plt.plot(range(1, n_components_max + 1), cumulative_explained_variance_ratio_)\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Explained Variance Ratio\")\n",
    "    plt.title(\"Explained Variance by PCA\")\n",
    "    plt.show()\n",
    "\n",
    "  # Calculate the second derivative\n",
    "  second_derivative = np.gradient(np.gradient(cumulative_explained_variance_ratio_))\n",
    "  # Find the index of the maximum of the second derivative\n",
    "  recommended_components_elbow = np.argmax(second_derivative)\n",
    "\n",
    "  # Alternative recommendation based on 90% explained variance\n",
    "  threshold = 0.9\n",
    "  recommended_components_90_pct = None  # Initialize outside the loop\n",
    "  for i, ratio in enumerate(cumulative_explained_variance_ratio_):\n",
    "    if ratio >= threshold:\n",
    "      recommended_components_90_pct = i + 1\n",
    "      break\n",
    "\n",
    "  print(f\"Recommended number of components (elbow method): {recommended_components_elbow}\")\n",
    "  print(f\"Recommended number of components (90% threshold): {recommended_components_90_pct}\")\n",
    "  \n",
    "  selected_components = min(recommended_components_elbow, recommended_components_90_pct)\n",
    "  pca = PCA(n_components=selected_components)\n",
    "  print(f\"Selected number of components: {selected_components}\")\n",
    "\n",
    "  return pca.fit_transform(data)\n",
    "\n",
    "\n",
    "def umap_wrapper(n_neighbors, min_dist, n_components, metric):\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric\n",
    "        )\n",
    "\n",
    "\n",
    "def umap_reduce_2d(data, n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine', plot2d=True, plot_diagnostics=True, preprocess_pca=True):\n",
    "    reducer = umap_wrapper(n_neighbors=n_neighbors,\n",
    "                           min_dist=min_dist,\n",
    "                           n_components=n_components,\n",
    "                           metric=metric\n",
    "                           )\n",
    "    if preprocess_pca:\n",
    "       data = pca_preprocessor(data)\n",
    "    reduced_data = reducer.fit_transform(data)\n",
    "    \n",
    "    if plot2d:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(reduced_data[:,0], reduced_data[:,1])\n",
    "        plt.title(label=f\"n_neighbors={n_neighbors} - min_dist={min_dist} - n_components={n_components} - metric={metric}\", fontsize=18)\n",
    "    \n",
    "    if plot_diagnostics:\n",
    "        # The \"hammer\" algorithm, in this context, bundles together similar connections (lines) in the visualization. \n",
    "        # This declutters the plot, making it easier to see the overall trends and major connections within the UMAP embedding.  \n",
    "        umap.plot.connectivity(reducer, edge_bundling='hammer')\n",
    "        umap.plot.diagnostic(reducer, diagnostic_type='pca')\n",
    "        umap.plot.diagnostic(reducer, diagnostic_type='vq')\n",
    "        # When the local dimension is high it represents points that UMAP will have a harder time embedding as well.\n",
    "        # While the local dimension plot can provide some insights, it cannot be solely relied upon to estimate the optimal number of dimensions for UMAP\n",
    "        local_dims = umap.plot.diagnostic(reducer, diagnostic_type='local_dim')\n",
    "    return reduced_data\n",
    "\n",
    "\n",
    "def umap_reduce(data, n_neighbors=30, min_dist=0.0, n_components=10, metric='cosine', preprocess_pca=True):\n",
    "    \"\"\"\n",
    "    When using UMAP for dimension reduction we want to select different parameters than if you were using it for visualization. \n",
    "    First of all we want a larger n_neighbors value, small values are more prone to producing fine grained cluster structure \n",
    "    that may be more a result of patterns of noise in the data than actual clusters. \n",
    "    Second it is beneficial to set min_dist to a very low value. Since we actually want to pack points together densely \n",
    "    a low value will help, as well as making cleaner separations between clusters.\n",
    "    \"\"\"\n",
    "    reducer = umap_wrapper(n_neighbors=n_neighbors,\n",
    "                           min_dist=min_dist,\n",
    "                           n_components=n_components,\n",
    "                           metric=metric\n",
    "                           )\n",
    "    \n",
    "    if preprocess_pca:\n",
    "       data = pca_preprocessor(data)\n",
    "    return reducer.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [ 5  3  5  5 -1 -1 -1  1  5 -1  5  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  2 -1 -1 -1  1 -1 -1 -1  4 -1  2 -1 -1  4 -1 -1 -1  3  3  4 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1  5  4 -1 -1 -1 -1 -1 -1  3 -1 -1 -1  5 -1  2  2  3\n",
      " -1 -1  5 -1 -1 -1 -1 -1  4  5  1 -1 -1  5 -1 -1  2  4 -1 -1 -1 -1 -1  2\n",
      "  5  5 -1 -1 -1  2  1  5 -1 -1  2 -1  2  5 -1 -1  5  3  4  4 -1 -1  4 -1\n",
      "  0 -1 -1 -1  2 -1 -1  2 -1 -1 -1  2  2 -1 -1 -1 -1  4 -1 -1  2 -1 -1 -1\n",
      "  2  5  0 -1 -1  5 -1 -1 -1  2 -1 -1 -1 -1 -1  5  2 -1 -1 -1 -1 -1  2  2\n",
      " -1  3 -1  2 -1 -1 -1 -1 -1  0 -1 -1  1 -1  5  5  5 -1 -1  2  2  2 -1  0\n",
      "  5  5 -1 -1 -1 -1  5 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1  5 -1 -1 -1  4  3\n",
      " -1 -1  1  0  2 -1  0  3  5  3  0  2 -1 -1  3 -1  3  2  4 -1  3  3 -1  4\n",
      " -1  5 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  3 -1 -1  5 -1 -1 -1  1  2 -1  0 -1\n",
      " -1 -1  3 -1 -1 -1 -1 -1  3 -1 -1  1 -1 -1  4  4  5  5  3 -1 -1 -1 -1 -1\n",
      " -1 -1  3 -1 -1 -1 -1 -1 -1 -1  3  0 -1 -1 -1  3 -1 -1  3  3  1  2 -1  5\n",
      " -1  3 -1 -1 -1 -1  2  3  5 -1  2 -1  5  5 -1 -1  2  3  4  2  5  5  4  4\n",
      "  3 -1  0 -1  3  2 -1 -1 -1 -1 -1  1 -1 -1  5  5  3 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  3  4 -1  4  4 -1  2  2 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  1  1  1 -1  1  1 -1 -1  5 -1 -1 -1 -1 -1 -1 -1 -1  5  1 -1 -1\n",
      " -1  5 -1  2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  5 -1 -1 -1 -1 -1  5 -1\n",
      " -1  5  1 -1  3 -1 -1 -1  1  5 -1 -1 -1 -1 -1 -1 -1  5 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  1  0 -1 -1 -1 -1 -1 -1 -1  5  5  5 -1  5 -1  5  2 -1 -1  5  4\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  3  5  4  4  2  3  2 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  4 -1  5  2  2  5 -1  5 -1 -1  5 -1 -1 -1 -1 -1  2 -1 -1  3 -1 -1\n",
      " -1  5 -1 -1 -1 -1  3 -1 -1 -1  5 -1 -1 -1  4  3  2 -1 -1 -1 -1  5 -1  2\n",
      " -1  3 -1  0 -1 -1  3  5 -1 -1 -1 -1 -1 -1  2 -1  4 -1 -1 -1  3 -1  4  1\n",
      "  2 -1  5  2  5  5 -1 -1 -1 -1  1 -1  4 -1  5  3 -1  3  2  3 -1  3  2  5\n",
      "  5 -1 -1 -1  2  5 -1 -1 -1 -1  2 -1 -1 -1 -1 -1 -1 -1 -1  4  3 -1 -1 -1\n",
      " -1  3 -1  1 -1 -1 -1  2  3  0 -1  5 -1 -1 -1 -1 -1 -1 -1 -1  5 -1 -1 -1\n",
      "  4 -1 -1  4 -1  3 -1  0 -1 -1 -1  4 -1 -1 -1  3 -1  2  4  4 -1 -1  3 -1\n",
      "  3  3 -1 -1 -1  3  5  5  3 -1  4  5 -1  3  3  5 -1 -1  2 -1 -1 -1  1 -1\n",
      " -1 -1 -1  4 -1  0 -1 -1 -1 -1  5 -1  3  2 -1 -1 -1 -1  5 -1  5 -1  5  2\n",
      "  5 -1 -1  5 -1 -1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1  3 -1  5  1 -1 -1\n",
      " -1 -1 -1 -1  1  5 -1 -1  2  1 -1 -1]\n",
      "reduced 2d: [[ 9.241277   2.9975076]\n",
      " [ 9.988817   8.430205 ]\n",
      " [ 9.663915   2.6781044]\n",
      " ...\n",
      " [12.8056965  7.201153 ]\n",
      " [10.300572   3.3390515]\n",
      " [12.132329   5.2977   ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# read file\n",
    "def get_most_recent_file(directory: str) -> str | None:\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory)]\n",
    "    files = [f for f in files if os.path.isfile(f)]\n",
    "    if not files:\n",
    "        return None\n",
    "    file_ctimes = [(f, os.path.getctime(f)) for f in files]\n",
    "    most_recent_file = sorted(file_ctimes, key=lambda x: x[1], reverse=True)[0][0]\n",
    "    return most_recent_file\n",
    "\n",
    "\n",
    "# fit_transform umap at optimal settings\n",
    "def umap_reduce(embeddings):\n",
    "    reducer = umap.UMAP(n_neighbors=30, min_dist=1, n_components=20, metric=\"cosine\")\n",
    "    return reducer.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "# fit_transform hdbscan at optimal settings\n",
    "def hdbscan_clustering(reduced_embeddings):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=10,\n",
    "                                min_samples=5,\n",
    "                                cluster_selection_epsilon=0.0,\n",
    "                                metric=\"euclidean\",\n",
    "                                cluster_selection_method=\"eom\", # leaf\n",
    "                                prediction_data=True)\n",
    "    return clusterer.fit_predict(reduced_embeddings)\n",
    "\n",
    "\n",
    "# fit_transform umap to 2d\n",
    "def umap_reduce_2d(embeddings):\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric=\"cosine\")\n",
    "    return reducer.fit_transform(embeddings)\n",
    "\n",
    "# apply cluster label to entries\n",
    "# save plots for git pages\n",
    "\n",
    "def daily_processing(filename=None):\n",
    "    if filename is None:\n",
    "        filename = get_most_recent_file(\"../data\")\n",
    "    with gzip.open(filename, \"r\") as file:\n",
    "        arxiv = json.loads(file.read().decode(\"utf-8\"))\n",
    "        embeddings = [d[\"embedding\"] for d in arxiv]\n",
    "        reduced_embeddings = umap_reduce(embeddings)\n",
    "        labels = hdbscan_clustering(reduced_embeddings)\n",
    "        reduced_embeddings_2d = umap_reduce_2d(embeddings)\n",
    "    for reduced_embedding_2d, entry in zip(reduced_embeddings_2d, arxiv):\n",
    "        entry[\"plot_embedding\"] = reduced_embedding_2d\n",
    "    for label, entry in zip(labels, arxiv):\n",
    "        entry[\"cluster\"] = label\n",
    "        \n",
    "\n",
    "daily_processing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novel2comic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
