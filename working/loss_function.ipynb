{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def compute_coherence_measurements(article_embeddings, cluster_assignments):\n",
    "    \"\"\"\n",
    "    Computes coherence measurements for each cluster based on cosine similarity between article embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    article_embeddings (numpy.ndarray): Array of article embeddings (each row represents an article).\n",
    "    cluster_assignments (numpy.ndarray): Array of cluster assignments for each article.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Array of coherence measurements for each cluster.\n",
    "    \"\"\"\n",
    "    num_clusters = np.max(cluster_assignments) + 1\n",
    "    coherence_measurements = []\n",
    "\n",
    "    for cluster_id in range(num_clusters):\n",
    "        cluster_indices = np.where(cluster_assignments == cluster_id)[0]\n",
    "        cluster_embeddings = article_embeddings[cluster_indices]\n",
    "        \n",
    "        # Compute pairwise cosine similarity between article embeddings within the cluster\n",
    "        if len(cluster_embeddings) > 1:\n",
    "            similarity_matrix = cosine_similarity(cluster_embeddings)\n",
    "            mean_similarity = np.mean(similarity_matrix[np.triu_indices(len(cluster_embeddings), k=1)])\n",
    "            coherence_measurements.append(mean_similarity)\n",
    "        else:\n",
    "            coherence_measurements.append(0.0)  # If only one article in the cluster, coherence is 0\n",
    "\n",
    "    return np.array(coherence_measurements)\n",
    "\n",
    "\n",
    "def compute_diversity_measurements(cluster_terms):\n",
    "    \"\"\"\n",
    "    Computes diversity measurements for each cluster based on the diversity of terms.\n",
    "    \n",
    "    Parameters:\n",
    "    cluster_terms (list): List of lists, where each inner list contains the terms for a cluster.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Array of diversity measurements for each cluster.\n",
    "    \"\"\"\n",
    "    diversity_measurements = []\n",
    "\n",
    "    for cluster in cluster_terms:\n",
    "        term_counts = Counter(cluster)\n",
    "        total_terms = sum(term_counts.values())\n",
    "        term_probs = [count / total_terms for count in term_counts.values()]\n",
    "        entropy = -np.sum([p * np.log(p) for p in term_probs if p > 0])  # Compute entropy\n",
    "        diversity_measurements.append(entropy)\n",
    "\n",
    "    return np.array(diversity_measurements)\n",
    "\n",
    "\n",
    "def custom_loss(coherence_measurements, diversity_measurements, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Creates a custom loss function balancing semantic coherence and topic diversity.\n",
    "    \n",
    "    Parameters:\n",
    "    coherence_measurements (numpy.ndarray): Array of coherence measurements for each cluster.\n",
    "    diversity_measurements (numpy.ndarray): Array of diversity measurements for each cluster.\n",
    "    alpha (float): Weight parameter controlling the balance between coherence and diversity.\n",
    "    \n",
    "    Returns:\n",
    "    float: Combined loss value.\n",
    "    \"\"\"\n",
    "    # Ensure coherence and diversity measurements are numpy arrays\n",
    "    coherence_measurements = np.array(coherence_measurements)\n",
    "    diversity_measurements = np.array(diversity_measurements)\n",
    "    \n",
    "    # Compute coherence loss as the negative mean coherence measurement\n",
    "    coherence_loss = -np.mean(coherence_measurements)\n",
    "    \n",
    "    # Compute diversity loss as the mean diversity measurement\n",
    "    diversity_loss = np.mean(diversity_measurements)\n",
    "    \n",
    "    # Combine coherence and diversity losses using alpha\n",
    "    combined_loss = alpha * coherence_loss + (1 - alpha) * diversity_loss\n",
    "    \n",
    "    return combined_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davies-Bouldin Index (DBI)\n",
    "Silhouette Score\n",
    "Calinski-Harabasz Index\n",
    "Adjusted Rand Index (ARI)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
