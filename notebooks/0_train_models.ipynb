{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import joblib\n",
    "import umap\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API key from environment variable\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "#gemini-1.0-pro\n",
    "model = genai.GenerativeModel(\"gemini-1.0-pro-001\", safety_settings={\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(directory: str) -> list[str] | None:\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory)]\n",
    "    files = [f for f in files if os.path.isfile(f)]\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_training_data() -> list[float]:\n",
    "    df = pd.DataFrame()\n",
    "    files = get_file_list(\"../data\")\n",
    "    for file in files:\n",
    "        df = pd.concat([df, pd.read_json(file, orient=\"records\")])\n",
    "    return df\n",
    "\n",
    "\n",
    "def random_select_or_all(lst, num_elements=1000):\n",
    "    if len(lst) <= num_elements:\n",
    "        return lst\n",
    "    else:\n",
    "        return random.sample(lst, num_elements)\n",
    "\n",
    "\n",
    "def umap_reducer(n_neighbors=30, min_dist=0.0, n_components=20, metric='cosine') -> umap.UMAP:\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric\n",
    "        )\n",
    "\n",
    "\n",
    "def umap_reducer_2d(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine') -> umap.UMAP:\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric\n",
    "        )\n",
    "\n",
    "\n",
    "def hdbscan_clusterer(min_cluster_size=250, min_samples=5, cluster_selection_epsilon=0.0,\n",
    "                      metric=\"euclidean\", cluster_selection_method=\"eom\", prediction_data=True) -> hdbscan.HDBSCAN:\n",
    "    return hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, \n",
    "                           min_samples=min_samples, \n",
    "                           cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "                           metric=metric, \n",
    "                           cluster_selection_method=cluster_selection_method, \n",
    "                           prediction_data=prediction_data\n",
    "                           )\n",
    "\n",
    "\n",
    "def get_cluster_descriptions(df: pd.DataFrame) -> list[str]:\n",
    "    unique_labels = df[\"labels\"].unique().tolist()\n",
    "    unique_labels.sort()\n",
    "    cluster_descriptions = {}\n",
    "    for ul in unique_labels:\n",
    "        titles = df[df[\"labels\"] == ul][\"title\"].to_list()\n",
    "        titles = random_select_or_all(titles)\n",
    "        prompt = \"\"\"Abstracts from Arxiv were clustered. All of the following articles are from one cluster.\n",
    "Provide three single words that describe the cluster. Do not use phrases. Do not use hyphenated words.\n",
    "Do not use compound words.\"\"\"\n",
    "\n",
    "        for i, title in enumerate(titles):\n",
    "            prompt = f\"{prompt}\\n Article {i}: {title}\"\n",
    "        print(f\"Label: {ul} - Num of titles: {len(titles)} - Token count: {model.count_tokens(prompt)}\")\n",
    "        model_results = model.generate_content(prompt)\n",
    "        model_results = model_results.text.replace(\"\\n\", \", \")\n",
    "        cluster_descriptions[ul] = model_results\n",
    "        time.sleep(3)\n",
    "    return cluster_descriptions\n",
    "\n",
    "\n",
    "def train_models():\n",
    "    df = get_training_data()\n",
    "    embeddings = df[\"embedding\"].to_list()\n",
    "\n",
    "    umap_model = umap_reducer()\n",
    "    umap_2d_model = umap_reducer_2d()\n",
    "    hdbscan_model = hdbscan_clusterer()\n",
    "\n",
    "    reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "    labels = hdbscan_model.fit_predict(reduced_embeddings)\n",
    "    df[\"labels\"] = labels\n",
    "    print(\"Number of observations:\", len(labels))\n",
    "    print(\"Number of noise entries:\", len([num for num in labels if num == -1]))\n",
    "    print(\"Number of labels:\", max(labels) + 2)\n",
    "    cluster_descriptions = get_cluster_descriptions(df)\n",
    "    print(cluster_descriptions)\n",
    "    umap_2d_model.fit(embeddings)\n",
    "    joblib.dump(umap_model, \"../models/umap_model.pkl\")\n",
    "    joblib.dump(umap_2d_model, \"../models/umap_2d_model.pkl\")\n",
    "    joblib.dump(hdbscan_model, \"../models/hdbscan_model.pkl\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 10896\n",
      "Number of noise entries: 3181\n",
      "Number of labels: 11\n",
      "Label: -1 - Num of titles: 1000 - Token count: total_tokens: 20776\n",
      "\n",
      "Label: 0 - Num of titles: 324 - Token count: total_tokens: 7230\n",
      "\n",
      "Label: 1 - Num of titles: 1000 - Token count: total_tokens: 21327\n",
      "\n",
      "Label: 2 - Num of titles: 332 - Token count: total_tokens: 6708\n",
      "\n",
      "Label: 3 - Num of titles: 830 - Token count: total_tokens: 18521\n",
      "\n",
      "Label: 4 - Num of titles: 443 - Token count: total_tokens: 10087\n",
      "\n",
      "Label: 5 - Num of titles: 555 - Token count: total_tokens: 11738\n",
      "\n",
      "Label: 6 - Num of titles: 578 - Token count: total_tokens: 12429\n",
      "\n",
      "Label: 7 - Num of titles: 1000 - Token count: total_tokens: 19576\n",
      "\n",
      "Label: 8 - Num of titles: 355 - Token count: total_tokens: 6779\n",
      "\n",
      "Label: 9 - Num of titles: 669 - Token count: total_tokens: 14767\n",
      "\n",
      "{-1: '- Diffusion, - Generative models, - Time series', 0: 'Wireless, Artificial Intelligence, Computing', 1: 'Language, Model, Logic', 2: 'Federated learning, Privacy, Security', 3: '- Neural Radiance Fields, - Gaussian Splatting, - Scene Understanding', 4: 'Segmentation, Medical Imaging, AI', 5: '- Diffusion, - Image, - Text-to-Image', 6: '- Vision, - Language, - Multimodal', 7: 'Cluster Analysis, Machine Learning, Quantum Computing', 8: 'reinforcement learning, control, game theory', 9: '- Planning, - Control, - Manipulation'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novel2comic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
