{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import feedparser\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from feedparser.util import FeedParserDict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define retrieval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv provides an rss feed for all current day articles \n",
    "def retrieve_arxiv_rss(section: str = \"cs\") -> FeedParserDict | None:\n",
    "    base_url = \"http://rss.arxiv.org/rss\"\n",
    "    feed_url = f\"{base_url}/{section}\"\n",
    "\n",
    "    try:\n",
    "        feed = feedparser.parse(feed_url)\n",
    "\n",
    "        # Check for errors in parsing\n",
    "        if feed.bozo:\n",
    "            raise feedparser.bozo_exception(feed.bozo_exception.getMessage())\n",
    "        \n",
    "        return feed\n",
    "\n",
    "    except (ValueError, IOError, feedparser.ParseError) as e:\n",
    "        print(f\"Error fetching feed entries: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_arxiv_rss(feed: feedparser.FeedParserDict) -> list[dict[str, any]]:\n",
    "        data = []\n",
    "\n",
    "        # Iterate over items and extract information\n",
    "        for item in feed.entries:\n",
    "            title = item.title\n",
    "            link = item.link\n",
    "            summary = item.summary\n",
    "            \n",
    "            # Extract abstract from the summary\n",
    "            if summary:\n",
    "                summary_parts = summary.split('\\nAbstract:')\n",
    "                abstract = summary_parts[1].strip() if len(summary_parts) > 1 else ''\n",
    "            else:\n",
    "                abstract = ''\n",
    "            \n",
    "            guid = item.id\n",
    "            category = ','.join(tag.term for tag in item.tags)\n",
    "            rights = item.rights\n",
    "            author = item.author\n",
    "            \n",
    "            # Append extracted information as a dictionary to the data list\n",
    "            data.append({\n",
    "                'title': title,\n",
    "                'abstract': abstract,\n",
    "                'guid': guid,\n",
    "                'category': category,\n",
    "                'link': link,\n",
    "                'rights': rights,\n",
    "                'creator': author,\n",
    "            })\n",
    "\n",
    "        return data\n",
    "    \n",
    "\n",
    "def get_arxiv_rss_name(feed: feedparser.FeedParserDict) -> str:\n",
    "        published_date = datetime.strptime(feed.feed.published, '%a, %d %b %Y %H:%M:%S %z')\n",
    "        published_date = published_date.strftime('%Y_%m_%d')\n",
    "        section = feed.feed.link.split(\"/\")[-1]\n",
    "        return f\"{published_date}_{section}\"\n",
    "\n",
    "\n",
    "def write_json(filename: str, parsed_feed: list[dict[str, any]]) -> None:\n",
    "    subdirectory = \"../data\"\n",
    "    os.makedirs(subdirectory, exist_ok=True)\n",
    "    filepath = os.path.join(subdirectory, filename)\n",
    "    print(f\"writing json file: {filepath}.json.gz\")\n",
    "    with gzip.open(f\"{filepath}.json.gz\", \"w\") as f:\n",
    "        f.write(json.dumps(parsed_feed).encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# valid sections = cs, econ, eess, math, astro-ph, cond-mat, gr-qc, hep-ex, \n",
    "#                  hep-lat, hep-ph, hep-th, math-ph, nlin, nucl-ex, \n",
    "#                  nucl-th, physics, quant-ph, q-bio, q-fin, stat \n",
    "def daily_processing(section: str=\"cs\") -> None:\n",
    "    feed = retrieve_arxiv_rss(section)\n",
    "    if feed is not None:\n",
    "        print(\"retrieved feed ... \")\n",
    "        filename = get_arxiv_rss_name(feed)\n",
    "        parsed_feed = parse_arxiv_rss(feed)\n",
    "        print(\"parsed feed ... \")\n",
    "        write_json(filename, parsed_feed)\n",
    "        print(\"wrote json file ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve Arxiv articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    daily_processing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novel2comic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
